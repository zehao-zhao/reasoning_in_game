# Complete Dataset & Benchmark Flow Explanation

## Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GAME GENERATION (One-time)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Create 100 random 3Ã—3 zero-sum games                                â”‚
â”‚  â€¢ Each game has payoff matrix U[i,j] = Row player's payoff            â”‚
â”‚  â€¢ Column player's payoff = -U[i,j] (zero-sum constraint)              â”‚
â”‚  â€¢ Payoffs randomly sampled from [-100, 100]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GAME SETUP: Compute Nash Equilibrium (One-time)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  For each game, solve 2 linear programs:                               â”‚
â”‚                                                                         â”‚
â”‚  1. ROW PLAYER'S STRATEGY (Nash mixed strategy Ïƒ_r):                   â”‚
â”‚     â€¢ Maximize minimum expected payoff                                  â”‚
â”‚     â€¢ Formula: max_Ïƒ min_j Î£_i U[i,j] * Ïƒ_r[i]                       â”‚
â”‚     â€¢ Result: Ïƒ_r = [pâ‚, pâ‚‚, pâ‚ƒ] (probability distribution)           â”‚
â”‚                                                                         â”‚
â”‚  2. COLUMN PLAYER'S STRATEGY (Nash mixed strategy Ïƒ_c):                â”‚
â”‚     â€¢ Minimize maximum expected loss                                    â”‚
â”‚     â€¢ Formula: min_Ïƒ max_i Î£_j U[i,j] * Ïƒ_c[j]                       â”‚
â”‚     â€¢ Result: Ïƒ_c = [qâ‚, qâ‚‚, qâ‚ƒ] (probability distribution)           â”‚
â”‚                                                                         â”‚
â”‚  3. NASH VALUE: v* = Ïƒ_r^T @ U @ Ïƒ_c                                  â”‚
â”‚     â€¢ Expected payoff when both play Nash equilibrium                   â”‚
â”‚                                                                         â”‚
â”‚  Key insight: Column player ALWAYS plays Ïƒ_c (fixed!)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        DATASET STRUCTURE: games.json (Saved once per experiment)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [                                                                       â”‚
â”‚    {                                                                     â”‚
â”‚      "game_id": 0,                                                      â”‚
â”‚      "payoff_matrix": [[-25.09, 90.14, 46.40],                          â”‚
â”‚                        [19.73, -68.80, -68.80],                        â”‚
â”‚                        [-88.38, 73.24, 20.22]],                        â”‚
â”‚      "nash_equilibrium_row": [0.553, 0.447, 0.0],    â† Row plays this  â”‚
â”‚      "nash_equilibrium_col": [0.720, 0.0, 0.280]     â† Column plays thisâ”‚
â”‚    },                                                                    â”‚
â”‚    ...100 games total...                                                â”‚
â”‚  ]                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BENCHMARK TRIAL LOOP: For each trial of each game                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  ğŸ“ STEP 1: QUERY LLM                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  Input:  Game matrix (formatted as text)                               â”‚
â”‚  Output: LLM chooses ONE action: 0, 1, or 2                            â”‚
â”‚                                                                         â”‚
â”‚  Example prompt:                                                        â”‚
â”‚  "You're the row player. Payoff matrix:                                â”‚
â”‚   Action 0: [-25.09,  90.14,  46.40]                                   â”‚
â”‚   Action 1: [19.73,  -68.80, -68.80]                                   â”‚
â”‚   Action 2: [-88.38,  73.24,  20.22]                                   â”‚
â”‚   Choose action 0, 1, or 2."                                            â”‚
â”‚                                                                         â”‚
â”‚  LLM response: "I choose action 0"                                      â”‚
â”‚  Parsed: llm_decision = 0                                               â”‚
â”‚                                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                         â”‚
â”‚  âš”ï¸  STEP 2: GAME OUTCOME - LLM vs Nash Opponent                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚  â€¢ LLM chose action: 0                                                  â”‚
â”‚  â€¢ Opponent plays Nash strategy: Ïƒ_c = [0.720, 0.0, 0.280]             â”‚
â”‚                                                                         â”‚
â”‚  LLM's strategy (pure action 0):                                        â”‚
â”‚    Ïƒ_llm = [1.0, 0.0, 0.0]  (100% probability on action 0)            â”‚
â”‚                                                                         â”‚
â”‚  Expected payoff for LLM:                                               â”‚
â”‚    LLM_value = Ïƒ_llm @ U @ Ïƒ_c                                          â”‚
â”‚               = [1.0, 0.0, 0.0] @ U @ [0.720, 0.0, 0.280]             â”‚
â”‚               = U[0,:] @ Ïƒ_c                                            â”‚
â”‚               = [-25.09*0.720 + 90.14*0.0 + 46.40*0.280]               â”‚
â”‚               = -18.06 + 0 + 12.99                                      â”‚
â”‚               = -5.07  â† What LLM got                                   â”‚
â”‚                                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                         â”‚
â”‚  ğŸ† STEP 3: BEST RESPONSE COMPUTATION                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚  What could LLM have gotten if it played OPTIMALLY against Ïƒ_c?        â”‚
â”‚                                                                         â”‚
â”‚  Compute payoff for each possible action against Ïƒ_c:                  â”‚
â”‚    BR[0] = U[0,:] @ Ïƒ_c = -18.06 + 0 + 12.99 = -5.07                  â”‚
â”‚    BR[1] = U[1,:] @ Ïƒ_c = 19.73*0.720 - 68.80*0.0 - 68.80*0.280       â”‚
â”‚           = 14.20 + 0 - 19.26 = -5.06                                  â”‚
â”‚    BR[2] = U[2,:] @ Ïƒ_c = -88.38*0.720 + 73.24*0.0 + 20.22*0.280      â”‚
â”‚           = -63.63 + 0 + 5.66 = -57.97                                 â”‚
â”‚                                                                         â”‚
â”‚  Best response action: argmax{-5.07, -5.06, -57.97} = 1               â”‚
â”‚  Best response value: -5.06  â† What LLM should have gotten             â”‚
â”‚                                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                         â”‚
â”‚  ğŸ“Š STEP 4: COMPUTE NASH GAP METRIC                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  Nash gap = Best Response Value - LLM Value                            â”‚
â”‚          = (-5.06) - (-5.07)                                            â”‚
â”‚          = 0.01                                                         â”‚
â”‚                                                                         â”‚
â”‚  Interpretation:                                                        â”‚
â”‚    â€¢ Nash gap = 0: LLM played optimally against Nash opponent          â”‚
â”‚    â€¢ Nash gap > 0: LLM played suboptimally (lost money)                â”‚
â”‚    â€¢ Nash gap = 50: LLM could have earned 50 more by playing BR       â”‚
â”‚                                                                         â”‚
â”‚  For 100 games Ã— 1 trial (what we ran):                                â”‚
â”‚    â€¢ Mean gap: 17.59 (on average, LLM suboptimal by 17.59)             â”‚
â”‚    â€¢ Median gap: ~0 (half the games played optimally)                  â”‚
â”‚    â€¢ Hard games: 16 games with gap > 50 (LLM very suboptimal)          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DATASET STRUCTURE: trials.json (Saved after all trials)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [                                                                       â”‚
â”‚    {                                                                     â”‚
â”‚      "game_id": 0,                                                      â”‚
â”‚      "trial_id": 0,                                                     â”‚
â”‚      "llm_decision": 0,              â† Action chosen by LLM             â”‚
â”‚      "llm_value": -5.067,            â† Payoff LLM achieved              â”‚
â”‚      "best_response_value": -5.067,  â† Best possible payoff             â”‚
â”‚      "nash_gap": 0.0                 â† Difference (optimality metric)   â”‚
â”‚    },                                                                    â”‚
â”‚    {                                                                     â”‚
â”‚      "game_id": 1,                                                      â”‚
â”‚      "trial_id": 0,                                                     â”‚
â”‚      "llm_decision": 0,              â† LLM chose action 0               â”‚
â”‚      "llm_value": -78.828,           â† But got -78.828 (bad!)           â”‚
â”‚      "best_response_value": -42.149, â† Could have gotten -42.149        â”‚
â”‚      "nash_gap": 36.679              â† Lost 36.679 by poor choice       â”‚
â”‚    },                                                                    â”‚
â”‚    ...100 trials total (1 per game)...                                 â”‚
â”‚  ]                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Conceptual Points

### 1. **The Opponent is ALWAYS Playing Nash**
- Column player strategy Ïƒ_c is computed ONCE during setup
- **Same Ïƒ_c is used for ALL trials of that game**
- LLM is measured against this fixed, optimal opponent
- This is intentional: we want to measure LLM performance against best-play

### 2. **LLM Gets ONE Choice Per Trial**
- LLM sees the game matrix and chooses a pure action (0, 1, or 2)
- No mixing: LLM cannot randomize
- For multiple trials, we query the LLM multiple times (gets different answers)
- This tests LLM consistency and robustness

### 3. **The Comparison Logic**
```
LLM's payoff:           Ïƒ_llm @ U @ Ïƒ_c  (vector @ matrix @ vector)
Best response payoff:   max_i (U[i,:] @ Ïƒ_c)  (best single action vs Nash)

Nash gap = BR payoff - LLM payoff

If gap = 0:   LLM played optimally
If gap > 0:   LLM was suboptimal by this amount
If gap = 100: LLM could have earned 100 more points
```

### 4. **Why We Use Nash Column Strategy**
- **Test hypothesis:** "Can LLM play game-theoretically sound strategies?"
- **Measuring against Nash** = measuring against best-possible opponent
- This isolates LLM's strategic understanding from opponent behavior
- If opponent plays Nash, LLM CANNOT do better than its best response

### 5. **The Three Values Explained**

| Value | What It Is | How Computed |
|-------|-----------|--------------|
| `llm_value` | What the LLM achieved | LLM's action vector @ payoff matrix @ Nash column strategy |
| `best_response_value` | Best possible against Nash | max over all actions of (action @ payoff matrix @ Nash strategy) |
| `nash_gap` | Suboptimality metric | Best response value - LLM value |

---

## Complete Example (Game 1)

```
Game Matrix U:
  Col0   Col1   Col2
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  19.73  -68.80 -68.80  â† Action 0
  19.73  -68.80 -68.80  â† Action 1  
  -88.38  73.24  20.22  â† Action 2

Nash Equilibrium (computed once):
  Row player should play: Ïƒ_r = [some mixture]
  Column player should play: Ïƒ_c = [0.72, 0.0, 0.28]

Trial 0:
  LLM is shown the matrix
  LLM response: "I choose action 0"
  
  Computation:
    LLM strategy:           Ïƒ_llm = [1.0, 0.0, 0.0]
    LLM value:              [1.0, 0.0, 0.0] @ U @ [0.72, 0.0, 0.28]
                          = 19.73*0.72 - 68.80*0.0 - 68.80*0.28
                          = 14.21 - 19.26 = -5.05
    
    Best response payoffs:
      BR[0] = 19.73*0.72 - 68.80*0 - 68.80*0.28 = -5.05
      BR[1] = 19.73*0.72 - 68.80*0 - 68.80*0.28 = -5.05
      BR[2] = -88.38*0.72 + 73.24*0 + 20.22*0.28 = -57.97
    
    Best response value = max{-5.05, -5.05, -57.97} = -5.05
    Nash gap = -5.05 - (-5.05) = 0.0 âœ“ (LLM played optimally!)

Trial 1 (same game, different LLM response):
  LLM is shown the same matrix again
  LLM response: "I choose action 2"  â† Different answer!
  
  Computation:
    LLM strategy:           Ïƒ_llm = [0.0, 0.0, 1.0]
    LLM value:              [0.0, 0.0, 1.0] @ U @ [0.72, 0.0, 0.28]
                          = -88.38*0.72 + 73.24*0 + 20.22*0.28
                          = -63.63 + 0 + 5.66 = -57.97
    
    Best response value = -5.05 (same as before)
    Nash gap = -5.05 - (-57.97) = 52.92 âœ— (LLM played poorly!)
```

---

## Summary

**The complete flow:**
1. **Generate** 100 random 3Ã—3 games
2. **Setup**: For each game, compute the Nash equilibrium (pure strategy pairs or mixed strategies)
3. **Key decision**: Column player ALWAYS plays its Nash strategy Ïƒ_c
4. **Each trial**: Query LLM, get one action, compute payoff against Ïƒ_c
5. **Measure**: Compare LLM's payoff to what LLM could have achieved (best response)
6. **Nash gap** quantifies: How many points LLM lost by not playing optimally

**The key insight:**
- By fixing column player to Nash strategy, we isolate pure strategic acumen
- LLM cannot "beat" the opponent through opponent mistakes
- Nash gap measures true game-theoretic understanding
- High gap = LLM doesn't understand the strategic structure of the game
